import random
from typing import Tuple, Optional, Annotated

from PIL import Image
from msgspec import Meta

from gsuid_core.logger import logger
from gsuid_core.models import Event
from gsuid_core.segment import MessageSegment
from gsuid_core.ai_core.register import ai_tools
from gsuid_core.utils.resource_manager import RM

from ._request import (
    draw_img_by_qwen_2512,
    gen_music_by_ace_step_1_5,
    gen_speech_by_index_tts_2,
    edit_img_by_qwen_edit_2511,
    gen_video_by_img_by_wan2_2,
    gen_video_by_text_by_wan2_2,
    draw_img_by_img_by_qwen_2512,
)
from ...utils.database.models import RHBind

text2image_workflow = {
    "qwen_2512": draw_img_by_qwen_2512,
}

image2image_workflow = {
    "qwen_2512_with_lora": draw_img_by_img_by_qwen_2512,
}

image_edit_workflow = {
    "qwen_2511": edit_img_by_qwen_edit_2511,
}

music_workflow = {
    "ace_step1.5": gen_music_by_ace_step_1_5,
}

speech_workflow = {
    "IndexTTS2": gen_speech_by_index_tts_2,
}

text2video_workflow = {
    "wan2.2_text2video": gen_video_by_text_by_wan2_2,
}

image2video_workflow = {
    "wan2.2_img2video": gen_video_by_img_by_wan2_2,
}


async def check_point(ev: Event, point: int) -> Tuple[bool, str]:
    """
    æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç§¯åˆ†
    """
    logger.info(f"[RHComfyUI] check_point: UserID:{ev.user_id} BotID:{ev.bot_id} Point:{point}")
    bind = await RHBind.deduct_point(
        ev.user_id,
        ev.bot_id,
        point,
    )
    if bind:
        return True, f"ğŸ’ª ç§¯åˆ†å……è¶³ï¼å·²æ‰£é™¤{point}ç§¯åˆ†!\nâœ… æ­£åœ¨ç”Ÿæˆï¼Œé¢„è®¡å°†ç­‰å¾…1åˆ†é’Ÿ..."
    else:
        return False, f"âŒ ç§¯åˆ†ä¸è¶³ï¼éœ€è¦{point}ç§¯åˆ†ï¼"


@ai_tools(check_func=check_point, point=1)
async def gen_image_by_text(
    prompt: Annotated[str, Meta(description="ç”Ÿæˆå›¾ç‰‡çš„æç¤ºè¯")],
    w: Annotated[int, Meta(description="ç”Ÿæˆå›¾ç‰‡çš„å®½åº¦")] = 720,
    h: Annotated[int, Meta(description="ç”Ÿæˆå›¾ç‰‡çš„é«˜åº¦")] = 1280,
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
) -> Image.Image:
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ–‡ç”Ÿå›¾çš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(text2image_workflow.keys()))

    model_func = text2image_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")
    result = await model_func(prompt, w, h)
    return result


@ai_tools(check_func=check_point, point=1)
async def gen_image_by_img(
    prompt: Annotated[str, Meta(description="ç”Ÿæˆå›¾ç‰‡çš„æç¤ºè¯")],
    image_id: Annotated[str, Meta(description="ç”Ÿæˆå›¾ç‰‡çš„åŸºç¡€å›¾ç‰‡ID")],
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
) -> Image.Image:
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œå›¾ç”Ÿå›¾çš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(image2image_workflow.keys()))

    model_func = image2image_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")

    image = await RM.get(image_id)
    result = await model_func(prompt, image)
    return result


@ai_tools(check_func=check_point, point=2)
async def gen_edit_img_by_img(
    prompt: Annotated[str, Meta(description="ç¼–è¾‘å›¾ç‰‡çš„æç¤ºè¯")],
    image_id_list: Annotated[list[str], Meta(description="ç¼–è¾‘å›¾ç‰‡çš„åŸºç¡€å›¾ç‰‡IDåˆ—è¡¨")],
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
):
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œå›¾ç‰‡ç¼–è¾‘çš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(image_edit_workflow.keys()))

    model_func = image_edit_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")

    image_list = [await RM.get(image_id) for image_id in image_id_list]
    result = await model_func(prompt, image_list)
    return result


@ai_tools(check_func=check_point, point=1)
async def gen_music(
    style_prompt: Annotated[str, Meta(description="ç”ŸæˆéŸ³ä¹çš„é£æ ¼æç¤ºè¯")],
    lyric_prompt: Annotated[Optional[str], Meta(description="ç”ŸæˆéŸ³ä¹çš„æ­Œè¯æç¤ºè¯")] = None,
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
):
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡ŒéŸ³ä¹ç”Ÿæˆçš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(music_workflow.keys()))

    model_func = music_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")
    result = await model_func(style_prompt, lyric_prompt)
    if result is not None:
        return MessageSegment.record(result)
    return result


@ai_tools(check_func=check_point, point=1)
async def gen_speech(
    text: Annotated[str, Meta(description="ç”Ÿæˆè¯­éŸ³çš„æ–‡æœ¬")],
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
):
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè¯­éŸ³ç”Ÿæˆçš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(speech_workflow.keys()))

    model_func = speech_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")
    result = await model_func(text)
    if result is not None:
        return MessageSegment.record(result)
    return result


@ai_tools(check_func=check_point, point=8)
async def gen_video_by_text(
    prompt: Annotated[str, Meta(description="ç”Ÿæˆè§†é¢‘çš„æç¤ºè¯")],
    w: Annotated[int, Meta(description="ç”Ÿæˆè§†é¢‘çš„å®½åº¦")] = 720,
    h: Annotated[int, Meta(description="ç”Ÿæˆè§†é¢‘çš„é«˜åº¦")] = 1280,
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
):
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ–‡ç”Ÿè§†é¢‘çš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(text2video_workflow.keys()))

    model_func = text2video_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")
    result = await model_func(prompt, w, h)
    if result is not None:
        return MessageSegment.video(result)
    return result


@ai_tools(check_func=check_point, point=8)
async def gen_video_by_img(
    prompt: Annotated[str, Meta(description="ç”Ÿæˆè§†é¢‘çš„æç¤ºè¯")],
    image_id: Annotated[str, Meta(description="ç”Ÿæˆè§†é¢‘çš„åŸºç¡€å›¾ç‰‡ID")],
    w: Annotated[int, Meta(description="ç”Ÿæˆè§†é¢‘çš„å®½åº¦")] = 720,
    h: Annotated[int, Meta(description="ç”Ÿæˆè§†é¢‘çš„é«˜åº¦")] = 1280,
    model: Annotated[Optional[str], Meta(description="ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¸ºç©ºæ—¶é»˜è®¤éšæœºé€‰æ‹©")] = None,
):
    """
    è°ƒç”¨AIæ¨¡å‹è¿›è¡Œå›¾ç”Ÿè§†é¢‘çš„å·¥å…·
    """
    if model is None:
        model = random.choice(list(image2video_workflow.keys()))

    model_func = image2video_workflow.get(model)
    if model_func is None:
        raise ValueError(f"æ¨¡å‹ {model} ä¸å­˜åœ¨")
    image = await RM.get(image_id)
    result = await model_func(prompt, image, w, h)
    if result is not None:
        return MessageSegment.video(result)
    return result
